# Megrez-3B-Omni: è½¯ç¡¬ååŒé‡Šæ”¾æ— ç©¹ç«¯ä¾§æ™ºèƒ½

<div align="center">

<p align="center">
    <img src="assets/megrez_logo.png" width="400"/>
<p>
<p align="center">
        ğŸ”— <a href="https://github.com/infinigence/Infini-Megrez-Omni">GitHub</a>&nbsp&nbsp | &nbsp&nbspğŸ  <a href="https://cloud.infini-ai.com/genstudio/model/mo-c73owqiotql7lozr">Infini-AI mass</a>&nbsp&nbsp | &nbsp&nbspğŸ“– <a href="https://cloud.infini-ai.com/assets/png/wechat_official_account.1f7e61401727063822266.png">WeChat Official</a>&nbsp&nbsp | &nbsp&nbspğŸ’¬ <a href="https://cloud.infini-ai.com/assets/png/wechat_community.7dbbc0b51727063822266.png">WeChat Groups</a>&nbsp&nbsp
</p>

<strong>ä¸­æ–‡ |
[English](./README.md)</strong>

</div>

## æ¨¡å‹ç®€ä»‹

**Megrez-3B-Omni** æ˜¯ç”±æ— é—®èŠ¯ç©¹ï¼ˆ[Infinigence AI](https://cloud.infini-ai.com/platform/ai)ï¼‰ç ”å‘çš„ç«¯ä¾§å…¨æ¨¡æ€ç†è§£æ¨¡å‹ï¼ŒåŸºäºæ— é—®å¤§è¯­è¨€æ¨¡å‹Megrez-3B-Instructæ‰©å±•ï¼ŒåŒæ—¶å…·å¤‡å›¾ç‰‡ã€æ–‡æœ¬ã€éŸ³é¢‘ä¸‰ç§æ¨¡æ€æ•°æ®çš„ç†è§£åˆ†æèƒ½åŠ›ï¼Œåœ¨ä¸‰ä¸ªæ–¹é¢å‡å–å¾—æœ€ä¼˜ç²¾åº¦

- åœ¨å›¾åƒç†è§£æ–¹é¢ï¼ŒåŸºäº SigLip-400Mæ„å»ºå›¾åƒTokenï¼Œç»¼åˆæ€§èƒ½è¶…è¶ŠLLaVA-NeXT-Yi-34Bç­‰æ›´å¤§å‚æ•°è§„æ¨¡çš„æ¨¡å‹ï¼Œæ˜¯åœ¨MME, MMVet, OCRBench, MMMUç­‰å¤šä¸ªä¸»æµæµ‹è¯•é›†ä¸Šç›®å‰ç²¾åº¦æœ€é«˜çš„å›¾åƒç†è§£æ¨¡å‹ä¹‹ä¸€ã€‚åœ¨åœºæ™¯ç†è§£ã€OCRç­‰æ–¹é¢å…·æœ‰è‰¯å¥½è¡¨ç°ã€‚
- åœ¨è¯­è¨€ç†è§£æ–¹é¢ï¼ŒMegrez-3B-Omniå¹¶æœªç‰ºç‰²æ¨¡å‹çš„æ–‡æœ¬å¤„ç†èƒ½åŠ›ï¼Œç»¼åˆèƒ½åŠ›è¾ƒå•æ¨¡æ€ç‰ˆæœ¬ï¼ˆMegrez-3B-Instructï¼‰ç²¾åº¦å˜åŒ–ä½äº2%ï¼Œä¿æŒåœ¨C-EVALã€MMLU (Proï¼‰ã€AlignBenchç­‰å¤šä¸ªæµ‹è¯•é›†ä¸Šçš„æœ€ä¼˜ç²¾åº¦ä¼˜åŠ¿ï¼Œä¾ç„¶å–å¾—è¶…è¶Šä¸Šä¸€ä»£14Bæ¨¡å‹çš„èƒ½åŠ›è¡¨ç°
- åœ¨è¯­éŸ³ç†è§£æ–¹é¢ï¼Œé‡‡ç”¨Whisper-large-v3çš„Encoderå¤´ï¼ˆå‚æ•°é‡~600Mï¼‰ï¼Œæ”¯æŒä¸­è‹±æ–‡è¯­éŸ³è¾“å…¥åŠå¤šè½®å¯¹è¯ï¼Œæ”¯æŒå¯¹è¾“å…¥å›¾ç‰‡çš„è¯­éŸ³æé—®ï¼Œæ ¹æ®è¯­éŸ³æŒ‡ä»¤ç›´æ¥å“åº”æ–‡æœ¬ï¼Œåœ¨å¤šé¡¹åŸºå‡†ä»»åŠ¡ä¸Šå–å¾—äº†é¢†å…ˆçš„ç»“æœ

## åŸºç¡€ä¿¡æ¯

<table>
  <thead>
    <tr>
      <th></th>
      <th>Language Module</th>
      <th>Vision Module</th>
      <th>Audio Module</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>æ¶æ„</td>
      <td>Llama-2 with GQA</td>
      <td>siglip-so400m</td>
      <td>Whisper-large-v3
(encoder-only)</td>
    </tr>
    <tr>
      <td># Params (Backbone)</td>
      <td>2.29B</td>
      <td>0.42B</td>
      <td>0.64B</td>
    </tr>
    <tr>
      <td>Connector</td>
      <td>-</td>
      <td>Cross Attention</td>
      <td>Linear</td>
    </tr>
    <tr>
      <td># Params (Others)</td>
      <td>Emb: 0.31B<br>Softmax: 0.31B</td>
      <td>Connector: 0.036B</td>
      <td>Connector: 0.003B</td>
    </tr>
    <tr>
      <td># Params (Total)</td>
      <td colspan="3">4B</td>
    </tr>
    <tr>
      <td># Vocab Size</td>
      <td>122880</td>
      <td>64 tokens/slice</td>
      <td>-</td>
    </tr>
    <tr>
      <td>Context length</td>
      <td colspan="3">4K tokens</td>
    </tr>
    <tr>
      <td>Supported languages</td>
      <td colspan="3">Chinese & English</td>
    </tr>
  </tbody>
</table>

### å›¾ç‰‡ç†è§£èƒ½åŠ›

![OpencompassBmk](assets/opencompass.jpg)

|         model         |       basemodel       |  å‘å¸ƒæ—¶é—´  | TFå‚æ•°é‡ï¼ˆBï¼‰ | æ€»å‚æ•°é‡ï¼ˆB) | OpenCompass (åœ¨çº¿) |   MME   | MMMU val |   OCRBench | Math-Vista-Mini | RealWorldQA | MMVet | hallusionBench | MMB TEST(en) | MMB TEST(zh) | TextVQA val | AI2D_TEST | MMstar | DocVQA_TEST |
|:---------------------:|:---------------------:|:----------:|:-------------:|:------------:|:------------------:|:-------:|:--------:|:----------:|:---------------:|:-----------:|:-----:|:--------------:|:------------:|:------------:|:-----------:|:---------:|:------:|:-----------:|
|     Megrez-3B-Omni    |       Megrez-3B       | 2024.12.16 |      2.29     |     3.376    |        65.5        | 2315.41 |   51.89  |    82.8    |        62       |    71.89    |  54.5 |      50.12     |     80.8     |     82.3     |     80.3    |   82.05   |  60.46 |    91.62    |
|         GPT-4o        |           -           | 2024.08.06 |       -       |       -      |        71.5        |  2328.7 |   69.2   |     736    |       61.3      |     75.4    |  69.1 |       55       |       -      |       -      |      -      |    84.6   |    -   |     92.8    |
|      GPT-4o mini      |           -           | 2024.08.06 |       -       |       -      |        64.1        |  2003.4 |    60    |     785    |       52.4      |     67.1    |  66.9 |      46.1      |       -      |       -      |      -      |    77.8   |    -   |      -      |
|  Qwen2-VL-2B-Instruct |       Qwen2-1.5B      | 2024.08.28 |      1.31     |     2.21     |        57.2        |   1872  |   41.1   |     794    |        43       |     62.9    |  49.5 |      41.7      |     74.9     |     73.5     |     79.7    |    74.7   |   48   |     90.1    |
|     InternVL2.5-2B    | Internlm2.5-1.8B-chat | 2024.12.06 |      1.89     |     2.21     |        59.9        |  2138.2 |   43.6   |     804    |       51.3      |     60.1    |  60.8 |      42.6      |     74.7     |     71.9     |     74.3    |    74.9   |  53.7  |     88.7    |
|      BlueLM-V-3B      |           -           | 2024.11.29 |      2.7      |      3.1     |        66.1        |    -    |   45.1   |     829    |       60.8      |     66.7    |  61.8 |       48       |      83      |     80.5     |     78.4    |    85.3   |  62.3  |     87.8    |
|     InternVL2.5-4B    |  Qwen2.5-3B-Instruct  | 2024.12.06 |      3.09     |     3.71     |        65.1        |  2337.5 |   52.3   |     828    |       60.5      |     64.3    |  60.6 |      46.3      |     81.1     |     79.3     |     76.8    |    81.4   |  58.3  |     91.6    |
|     Baichuan-Omni     |       Unknown-7B      | 2024.10.11 |       7       |       7      |          -         |  2186.9 |   47.3   |     700    |       51.9      |     62.6    |  65.4 |      47.8      |     76.2     |     74.9     |     74.3    |     -     |    -   |      -      |
|     MiniCPM-V-2.6     |        Qwen2-7B       | 2024.08.06 |      6.5      |      8.1     |        65.2        |  2348.4 |   49.8   |     852    |       60.6      |     69.7    |   60  |      48.1      |     81.2     |      79      |     80.1    |    82.1   |  57.26 |     90.8    |
|  Qwen2-VL-7B-Instruct |        Qwen2-7B       | 2024.08.28 |      6.5      |     8.29     |         67         |  2326.8 |   54.1   |     845    |       58.2      |     70.1    |   62  |      50.6      |      83      |     80.5     |     84.3    |     83    |  60.7  |     94.5    |
|  MiniCPM-Llama3-V-2.5 |   Llama3-Instruct 8B  | 2024.05.20 |       8       |     8.54     |        58.8        |  2024.6 |   45.8   |     725    |       54.3      |     63.5    |  52.8 |      42.4      |     77.2     |     74.2     |     76.6    |    78.4   |    -   |     84.8    |
|          VITA         |      Mixtral 8x7B     | 2024.08.12 |      12.9     |     12.9     |          -         |   2097  |   47.3   |     678    |       44.9      |      59     |  41.6 |      39.7      |     74.7     |     71.4     |     71.8    |     -     |    -   |      -      |
|       GLM-4V-9B       |        GLM-4-9B       | 2024.06.04 |      8.2      |     13.9     |        59.1        |  2018.8 |   46.9   |     776    |       51.1      |      -      |   58  |      46.6      |     81.1     |     79.4     |      -      |    81.1   |  58.7  |      -      |
|   LLaVA-NeXT-Yi-34B   |         Yi-34B        | 2024.01.18 |       34      |      34      |         55         |  2006.5 |   48.8   |     574    |       40.4      |      66     |  50.7 |      34.8      |     81.1     |      79      |     69.3    |    78.9   |  51.6  |      -      |
| Qwen2-VL-72B-Instruct |       Qwen2-72B       | 2024.08.28 |     70.21     |     73.4     |        74.8        |  2482.7 |   64.5   |     877    |       70.5      |     77.8    |   74  |      58.1      |     86.5     |     86.6     |     85.5    |    88.1   |  68.3  |     96.5    |

### æ–‡æœ¬å¤„ç†èƒ½åŠ›

|                       |          |             |                                       | å¯¹è¯&æŒ‡ä»¤ |                 |        | ä¸­æ–‡&è‹±æ–‡ä»»åŠ¡ |            |       |          |  ä»£ç ä»»åŠ¡ |       | æ•°å­¦ä»»åŠ¡ |       |
|:---------------------:|:--------:|:-----------:|:-------------------------------------:|:---------:|:---------------:|:------:|:-------------:|:----------:|:-----:|:--------:|:---------:|:-----:|:--------:|:-----:|
|         models        | æŒ‡ä»¤æ¨¡å‹ |   å‘å¸ƒæ—¶é—´  | Transformerå‚æ•°é‡ ï¼ˆä¸å«emb&softmaxï¼‰ |  MT-Bench | AlignBench (ZH) | IFEval |  C-EVAL (ZH)  | CMMLU (ZH) | MMLU  | MMLU-Pro | HumanEval |  MBPP |   GSM8K  |  MATH |
| Megrez-3B-Omni        |     Y    |  2024.12.16 |                  2.3                  |    8.4    |       6.5       |  66.5  |     84.0      |    75.3    | 73.3  |   45.2   |   72.6    | 60.6  |   63.8   | 27.3  |
| Megrez-3B-Instruct    |     Y    |  2024.12.16 |                  2.3                  |   8.64    |      7.06       |  68.6  |     84.8      |    74.7    | 72.8  |   46.1   |   78.7    | 71.0  |   65.5   | 28.3  |
| Baichuan-Omni         |     Y    |  2024.10.11 |                  7.0                  |     -     |        -        |    -   |     68.9      |    72.2    |  65.3 |     -    |     -     |   -   |     -    |   -   |
| VITA                  |     Y    |  2024.08.12 |                 12.9                  |     -     |        -        |    -   |     56.7      |    46.6    | 71.0  |     -    |     -     |   -   |   75.7   |   -   |
| Qwen1.5-7B            |          |  2024.02.04 |                  6.5                  |     -     |        -        |    -   |     74.1      |    73.1    | 61.0  |   29.9   |   36.0    | 51.6  |   62.5   | 20.3  |
| Qwen1.5-7B-Chat       |     Y    |  2024.02.04 |                  6.5                  |   7.60    |      6.20       |    -   |     67.3      |      -     | 59.5  |   29.1   |   46.3    | 48.9  |   60.3   | 23.2  |
| Qwen1.5-14B           |          |  2024.02.04 |                 12.6                  |     -     |        -        |    -   |     78.7      |    77.6    | 67.6  |     -    |   37.8    | 44.0  |   70.1   | 29.2  |
| Qwen1.5-14B-Chat      |     Y    |  2024.02.04 |                 12.6                  |    7.9    |        -        |    -   |       -       |      -     |   -   |     -    |     -     |   -   |     -    |   -   |
| Qwen2-7B              |          |  2024.06.07 |                  6.5                  |     -     |        -        |    -   |     83.2      |    83.9    | 70.3  |   40.0   |   51.2    | 65.9  |   79.9   | 44.2  |
| Qwen2-7b-Instruct     |     Y    |  2024.06.07 |                  6.5                  |   8.41    |      7.21       |  51.4  |     80.9      |    77.2    | 70.5  |   44.1   |   79.9    | 67.2  |   85.7   | 52.9  |
| Qwen2.5-3B-Instruct   |     Y    |  2024.9.19  |                  2.8                  |     -     |        -        |    -   |       -       |      -     |   -   |   43.7   |   74.4    | 72.7  |   86.7   | 65.9  |
| Qwen2.5-7B            |          |  2024.9.19  |                  6.5                  |     -     |        -        |    -   |       -       |      -     | 74.2  |   45.0   |   57.9    | 74.9  |   85.4   | 49.8  |
| Qwen2.5-7B-Instruct   |     Y    |  2024.09.19 |                  6.5                  |   8.75    |        -        |  74.9  |       -       |      -     |   -   |   56.3   |   84.8    | 79.2  |   91.6   | 75.5  |
| Llama-3.1-8B          |          |  2024.07.23 |                  7.0                  |    8.3    |       5.7       |  71.5  |     55.2      |    55.8    | 66.7  |   37.1   |     -     |   -   |   84.5   | 51.9  |
| Llama-3.2-3B          |          |  2024.09.25 |                  2.8                  |     -     |        -        |  77.4  |       -       |      -     | 63.4  |     -    |     -     |   -   |   77.7   | 48.0  |
| Phi-3.5-mini-instruct |     Y    |  2024.08.23 |                  3.6                  |    8.6    |       5.7       |  49.4  |     46.1      |    46.9    | 69.0  |   47.4   |   62.8    | 69.6  |   86.2   | 48.5  |
| MiniCPM3-4B           |     Y    |  2024.09.05 |                  3.9                  |   8.41    |      6.74       |  68.4  |     73.6      |    73.3    | 67.2  |     -    |   74.4    | 72.5  |   81.1   | 46.6  |
| Yi-1.5-6B-Chat        |     Y    |  2024.05.11 |                  5.5                  |   7.50    |      6.20       |    -   |     74.2      |    74.7    | 61.0  |     -    |   64.0    | 70.9  |   78.9   | 40.5  |
| GLM-4-9B-chat         |     Y    |  2024.06.04 |                  8.2                  |   8.35    |      7.01       |  64.5  |     75.6      |    71.5    | 72.4  |     -    |   71.8    |   -   |   79.6   | 50.6  |
| Baichuan2-13B-Base    |          |  2023.09.06 |                 12.6                  |     -     |      5.25       |    -   |     58.1      |    62.0    | 59.2  |     -    |   17.1    | 30.2  |   52.8   | 10.1  |

- Qwen2-1.5Bæ¨¡å‹çš„æŒ‡æ ‡åœ¨è®ºæ–‡å’ŒQwen2.5æŠ¥å‘Šä¸­ç‚¹æ•°ä¸ä¸€è‡´ï¼Œå½“å‰é‡‡ç”¨åŸå§‹è®ºæ–‡ä¸­çš„ç²¾åº¦

### è¯­éŸ³ç†è§£èƒ½åŠ›

|       Model      |     Base model     | Realease Time | Fleurs test-zh | WenetSpeech test_net | WenetSpeech test_meeting |
|:----------------:|:------------------:|:-------------:|:--------------:|:--------------------:|:------------------------:|
| Whisper-large-v3 |          -         |   2023.11.06  |      12.4      |         17.5         |           30.8           |
|  Qwen2-Audio-7B  |      Qwen2-7B      |   2024.08.09  |        9       |          11          |           10.7           |
|  Baichuan2-omni  |     Unknown-7B     |   2024.10.11  |        7       |          6.9         |            8.4           |
|       VITA       |    Mixtral 8x7B    |   2024.08.12  |        -       |      -/12.2(CER)     |        -/16.5(CER)       |
|  Megrez-3B-Omni  | Megrez-3B-Instruct |   2024.12.16  |      10.8      |         45.08        |           16.44          |

### é€Ÿåº¦

|                | image_tokens | prefill (tokens/s) | decode (tokens/s) |
|----------------|:------------:|:------------------:|:-----------------:|
| Megrez-3B-Omni |      448     |       6312.66      |       1294.9      |
| Qwen2-VL-2B    |     1378     |       7349.39      |       685.66      |
| MiniCPM-V-2_6  |      448     |       2167.09      |       452.51      |

å®éªŒè®¾ç½®ï¼š

- æµ‹è¯•ç¯å¢ƒä¸ºNVIDIA H100ä¸‹VLLMä¸‹è¾“å…¥128ä¸ªText tokenå’Œä¸€å¼  720*1480çš„å›¾ç‰‡ï¼Œè¾“å‡º128ä¸ªtokenï¼Œnum_seqså›ºå®šä¸º8ã€‚
- Qwen2-VL-2Bçš„åœ¨æ­¤å®éªŒä¸‹çš„decodeé€Ÿåº¦å°äºMegrez-3B-Omniï¼Œè™½ç„¶å…¶å…·å¤‡æ›´å°çš„åŸºåº§LLMï¼Œä½†æ˜¯ç¼–ç ä¸Šè¿°å¤§å°å›¾ç‰‡åçš„image_tokenç›¸è¾ƒMegrez-3B-Omniè¾ƒå¤šï¼Œå½±å“å®é™…æ¨ç†é€Ÿåº¦ã€‚

## å®‰è£…

ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å®‰è£…ä¾èµ–ï¼š

```shell
pip install -r requirements.txt
```

## å¾®è°ƒæ¨¡å‹

{{TBD}}

## æ¨ç†

### ä½¿ç”¨å¤šæ¨¡æ€æ•°æ®è¿›è¡Œå¤šè½®å¯¹è¯

è¯·ä½¿ç”¨å¦‚ä¸‹è„šæœ¬è¿›è¡Œæ¨ç†ã€‚è¯·å°† `PATH_TO_PRETRAINED_MODEL` æ›¿æ¢ä¸ºä¸‹è½½çš„æ¨¡å‹æƒé‡çš„è·¯å¾„ã€‚

```python
import torch
from transformers import AutoModelForCausalLM

path = "{{PATH_TO_PRETRAINED_MODEL}}"  # æ›´æ”¹ä¸ºæ¨¡å‹çš„è·¯å¾„

model = (
    AutoModelForCausalLM.from_pretrained(
        path,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
    )
    .eval()
    .cuda()
)

messages = [
    {
        "role": "user",
        "content": {
            "text": "Please describe the content of the image.",
            "image": "./data/sample_image.jpg",
        },
    },
]

MAX_NEW_TOKENS = 100
response = model.chat(
    messages,
    sampling=False,
    max_new_tokens=MAX_NEW_TOKENS,
)
print(response)
```

å®Œæ•´çš„å®ä¾‹è§ï¼š[example_chat_hf.py](example_chat_hf.py).

### ä½¿ç”¨ vLLM è¿›è¡Œæ¨ç†

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªåŸºäº vLLM æ¡†æ¶çš„æ¨ç†å‚è€ƒå®ç°ã€‚æ‚¨å¯ä»¥åœ¨ [vllm_demo/megrezo.py](vllm_demo/megrezo.py) ä¸­æ‰¾åˆ°æ¨¡å‹å®šä¹‰ã€‚

æ¨ç†æ­¥éª¤å¦‚ä¸‹ï¼š

1. å®‰è£… vLLM

æ³¨æ„ï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…ç‰¹å®šç‰ˆæœ¬çš„ä¾èµ–ï¼š

```shell
pip install vllm==0.6.3.post1 flash_attn==2.5.8 xformers==0.0.27.post2
```

2. è¿è¡Œæ¨ç†è„šæœ¬

vLLM å°šæœªæ­£å¼æ”¯æŒ MegrezOï¼Œå› æ­¤æ‚¨éœ€è¦å…ˆå¯¼å…¥æˆ‘ä»¬å®šä¹‰çš„æ¨¡å—ï¼š

```python
from vllm import ModelRegistry
from megrezo import MegrezOModel

ModelRegistry.register_model("MegrezO", MegrezOModel)
```

ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç è¿è¡Œæ¨ç†ï¼š

```python
from PIL import Image
from vllm import LLM
from vllm import SamplingParams


model_path = "{{PATH_TO_HF_PRETRAINED_MODEL}}"  # æ›´æ”¹ä¸ºæ¨¡å‹çš„è·¯å¾„
llm = LLM(
    model_path,
    trust_remote_code=True,
    gpu_memory_utilization=0.5,
)

sampling_params = SamplingParams(
    temperature=0,
    max_tokens=1000,
    repetition_penalty=1.2,
    stop=["<|turn_end|>", "<|eos|>"],
)

img = Image.open("../data/sample_image.jpg")

conversation = [
    {
        "role": "user",
        "content": {
            "text": "å›¾ç‰‡çš„å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "image": img,
        },
    },
]

# å°†å¯¹è¯è½¬æ¢ä¸º vLLM å¯æ¥å—çš„æ ¼å¼ã€‚
prompt = llm.get_tokenizer().apply_chat_template(
    conversation,
    tokenize=False,
    add_generation_prompt=True,
)
vllm_inputs = [
    {
        "prompt": prompt,
        "multi_modal_data": {
            "image": img,
        },
    }
]

# ç”Ÿæˆè¾“å‡º
outputs = llm.generate(
    vllm_inputs,
    sampling_params,
)

# æ‰“å°è¾“å‡º
for output in outputs:
    print(output.outputs[0].text)
```

å®Œæ•´çš„ç¤ºä¾‹è§ï¼š[vllm_demo/example_infer_vllm.py](vllm_demo/example_infer_vllm.py).

## ä½¿ç”¨ Gradio ä¸ MegrezO å¯¹è¯

æˆ‘ä»¬æä¾›åŸºäº Hugging Face Gradio <a href='https://github.com/gradio-app/gradio'><img src='https://img.shields.io/github/stars/gradio-app/gradio'></a> å®ç°çš„åœ¨çº¿å’Œæœ¬åœ° Demoã€‚

### åœ¨çº¿ Demo

æ¬¢è¿è¯•ç”¨åœ¨çº¿ Demo: {{TBD}}ã€‚

### æœ¬åœ° Demo
  
ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤éƒ¨ç½²æœ¬åœ° Gradio åº”ç”¨ï¼š

1. å®‰è£…ä¾èµ–:

```shell
pip install -r requirements.txt
```

2. å¯åŠ¨ Gradio åº”ç”¨

æ‚¨éœ€è¦åœ¨å‘½ä»¤è¡Œä¸­æŒ‡å®š `model_path` å’Œ `port`ã€‚`model_path` æ˜¯æ¨¡å‹çš„è·¯å¾„ï¼Œ`port` æ˜¯æœ¬åœ°æœåŠ¡å™¨çš„ç«¯å£å·ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œ`port` æ˜¯ `7860`ã€‚

```shell
python gradio_app.py --model_path {model_path} --port {port}
```

ç„¶åï¼Œæ‚¨å¯ä»¥åœ¨æµè§ˆå™¨ä¸­è®¿é—® `http://localhost:7860` ä¸æ¨¡å‹å¯¹è¯ã€‚

å¦‚éœ€è‡ªå®šä¹‰è¾“å…¥å’Œè¾“å‡ºæ¥å£ï¼Œè¯·ä¿®æ”¹ `gradio_app.py`ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è€ƒ [Gradio æ–‡æ¡£](https://gradio.app/docs)ã€‚
