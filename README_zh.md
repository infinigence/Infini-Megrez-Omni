<div align="center">

# Megrez-3B-Omni: è½¯ç¡¬ååŒé‡Šæ”¾æ— ç©¹ç«¯ä¾§æ™ºèƒ½

<p align="center">
    <img src="assets/megrez_logo.png" width="400"/>
<p>
<p align="center">
        ğŸ¤— <a href="https://huggingface.co/Infinigence/Megrez-3B-Omni">Huggingface</a>&nbsp&nbsp | &nbsp&nbspğŸ¤–<a href="https://www.modelscope.cn/models/InfiniAI/Megrez-3B-Omni">Modelscope</a>&nbsp&nbsp | &nbsp&nbspğŸ–¥ï¸ <a href="https://huggingface.co/Infinigence/Megrez-3B-Omni">Demo</a>&nbsp&nbsp | &nbsp&nbspğŸ“– <a href="https://cloud.infini-ai.com/assets/png/wechat_official_account.1f7e61401727063822266.png">WeChat Official</a>&nbsp&nbsp | &nbsp&nbspğŸ’¬ <a href="https://cloud.infini-ai.com/assets/png/wechat_community.7dbbc0b51727063822266.png">WeChat Groups</a>&nbsp&nbsp
</p>

<strong>ä¸­æ–‡ | [English](./README.md)</strong>

</div>

## æ¨¡å‹ç®€ä»‹

**Megrez-3B-Omni** æ˜¯ç”±æ— é—®èŠ¯ç©¹ï¼ˆ[Infinigence AI](https://cloud.infini-ai.com/platform/ai)ï¼‰ç ”å‘çš„**ç«¯ä¾§å…¨æ¨¡æ€**ç†è§£æ¨¡å‹ï¼ŒåŸºäºæ— é—®å¤§è¯­è¨€æ¨¡å‹Megrez-3B-Instructæ‰©å±•ï¼ŒåŒæ—¶å…·å¤‡å›¾ç‰‡ã€æ–‡æœ¬ã€éŸ³é¢‘ä¸‰ç§æ¨¡æ€æ•°æ®çš„ç†è§£åˆ†æèƒ½åŠ›ï¼Œåœ¨ä¸‰ä¸ªæ–¹é¢å‡å–å¾—æœ€ä¼˜ç²¾åº¦

- åœ¨å›¾åƒç†è§£æ–¹é¢ï¼ŒåŸºäº SigLip-400M æ„å»ºå›¾åƒTokenï¼Œåœ¨ OpenCompass æ¦œå•ä¸Šï¼ˆç»¼åˆ8ä¸ªä¸»æµå¤šæ¨¡æ€è¯„æµ‹åŸºå‡†ï¼‰å¹³å‡å¾—åˆ†66.2ï¼Œè¶…è¶Š LLaVA-NeXT-Yi-34B ç­‰æ›´å¤§å‚æ•°è§„æ¨¡çš„æ¨¡å‹ã€‚Megrez-3B-Omniä¹Ÿæ˜¯åœ¨MMEã€MMMUã€OCRBenchç­‰å¤šä¸ªä¸»æµæµ‹è¯•é›†ä¸Šç›®å‰ç²¾åº¦æœ€é«˜çš„å›¾åƒç†è§£æ¨¡å‹ä¹‹ä¸€ï¼Œåœ¨åœºæ™¯ç†è§£ã€OCRç­‰æ–¹é¢å…·æœ‰è‰¯å¥½è¡¨ç°ã€‚
- åœ¨è¯­è¨€ç†è§£æ–¹é¢ï¼ŒMegrez-3B-Omni å¹¶æœªç‰ºç‰²æ¨¡å‹çš„æ–‡æœ¬å¤„ç†èƒ½åŠ›ï¼Œç»¼åˆèƒ½åŠ›è¾ƒå•æ¨¡æ€ç‰ˆæœ¬ï¼ˆMegrez-3B-Instructï¼‰ç²¾åº¦å˜åŒ–å°äº2%ï¼Œä¿æŒåœ¨C-EVALã€MMLU (Proï¼‰ã€AlignBenchç­‰å¤šä¸ªæµ‹è¯•é›†ä¸Šçš„æœ€ä¼˜ç²¾åº¦ä¼˜åŠ¿ï¼Œä¾ç„¶å–å¾—è¶…è¶Šä¸Šä¸€ä»£14Bæ¨¡å‹çš„èƒ½åŠ›è¡¨ç°
- åœ¨è¯­éŸ³ç†è§£æ–¹é¢ï¼Œé‡‡ç”¨Whisper-large-v3çš„Encoderä½œä¸ºè¯­éŸ³è¾“å…¥ï¼Œæ”¯æŒä¸­è‹±æ–‡è¯­éŸ³è¾“å…¥åŠå¤šè½®å¯¹è¯ï¼Œæ”¯æŒå¯¹è¾“å…¥å›¾ç‰‡çš„è¯­éŸ³æé—®ï¼Œæ ¹æ®è¯­éŸ³æŒ‡ä»¤ç›´æ¥å“åº”æ–‡æœ¬ï¼Œåœ¨å¤šé¡¹åŸºå‡†ä»»åŠ¡ä¸Šå–å¾—äº†é¢†å…ˆçš„ç»“æœ

?????
æ”¾ä¸€ä¸‹é›·è¾¾å›¾å’Œç²¾åº¦æ•£ç‚¹å›¾ï¼Ÿï¼Ÿï¼Ÿï¼Ÿç„¶åå†™ä¸€å¥è¯¦ç»†ç²¾åº¦è§hfï¼Œä¸‹é¢ä¸»è¦å†™éƒ¨ç½²

ç•™ä¸€ä¸ªè§†é¢‘æˆ–è€…gif demoçš„ç©ºä½

ç¯å¢ƒç‰ˆæœ¬é‚£é‡Œhighlightä¸€ä¸‹ï¼Œæ³¨æ˜å…¶ä»–ç‰ˆæœ¬å­˜åœ¨é£é™©ï¼Œæœ‰é—®é¢˜æissue

gradio demoæ”¾ä¸€ä¸ªæ•ˆæœå›¾

## åŸºç¡€ä¿¡æ¯

<table>
  <thead>
    <tr>
      <th></th>
      <th>Language Module</th>
      <th>Vision Module</th>
      <th>Audio Module</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>æ¶æ„</td>
      <td>Llama-2 with GQA</td>
      <td>siglip-so400m</td>
      <td>Whisper-large-v3
(encoder-only)</td>
    </tr>
    <tr>
      <td># Params (Backbone)</td>
      <td>2.29B</td>
      <td>0.42B</td>
      <td>0.64B</td>
    </tr>
    <tr>
      <td>Connector</td>
      <td>-</td>
      <td>Cross Attention</td>
      <td>Linear</td>
    </tr>
    <tr>
      <td># Params (Others)</td>
      <td>Emb: 0.31B<br>Softmax: 0.31B</td>
      <td>Connector: 0.036B</td>
      <td>Connector: 0.003B</td>
    </tr>
    <tr>
      <td># Params (Total)</td>
      <td colspan="3">4B</td>
    </tr>
    <tr>
      <td># Vocab Size</td>
      <td>122880</td>
      <td>64 tokens/slice</td>
      <td>-</td>
    </tr>
    <tr>
      <td>Context length</td>
      <td colspan="3">4K tokens</td>
    </tr>
    <tr>
      <td>Supported languages</td>
      <td colspan="3">Chinese & English</td>
    </tr>
  </tbody>
</table>

### å›¾ç‰‡ç†è§£èƒ½åŠ›

![OpencompassBmk](assets/opencompass.jpg)

|         model         |       basemodel       |  å‘å¸ƒæ—¶é—´  | TFå‚æ•°é‡ï¼ˆBï¼‰ | æ€»å‚æ•°é‡ï¼ˆB) | OpenCompass (åœ¨çº¿) |   MME   | MMMU val |   OCRBench | Math-Vista-Mini | RealWorldQA | MMVet | hallusionBench | MMB TEST(en) | MMB TEST(zh) | TextVQA val | AI2D_TEST | MMstar | DocVQA_TEST |
|:---------------------:|:---------------------:|:----------:|:-------------:|:------------:|:------------------:|:-------:|:--------:|:----------:|:---------------:|:-----------:|:-----:|:--------------:|:------------:|:------------:|:-----------:|:---------:|:------:|:-----------:|
|     Megrez-3B-Omni    |       Megrez-3B       | 2024.12.16 |      2.29     |     3.376    |        65.5        | 2315.41 |   51.89  |    82.8    |        62       |    71.89    |  54.5 |      50.12     |     80.8     |     82.3     |     80.3    |   82.05   |  60.46 |    91.62    |
|         GPT-4o        |           -           | 2024.08.06 |       -       |       -      |        71.5        |  2328.7 |   69.2   |     736    |       61.3      |     75.4    |  69.1 |       55       |       -      |       -      |      -      |    84.6   |    -   |     92.8    |
|      GPT-4o mini      |           -           | 2024.08.06 |       -       |       -      |        64.1        |  2003.4 |    60    |     785    |       52.4      |     67.1    |  66.9 |      46.1      |       -      |       -      |      -      |    77.8   |    -   |      -      |
|  Qwen2-VL-2B-Instruct |       Qwen2-1.5B      | 2024.08.28 |      1.31     |     2.21     |        57.2        |   1872  |   41.1   |     794    |        43       |     62.9    |  49.5 |      41.7      |     74.9     |     73.5     |     79.7    |    74.7   |   48   |     90.1    |
|     InternVL2.5-2B    | Internlm2.5-1.8B-chat | 2024.12.06 |      1.89     |     2.21     |        59.9        |  2138.2 |   43.6   |     804    |       51.3      |     60.1    |  60.8 |      42.6      |     74.7     |     71.9     |     74.3    |    74.9   |  53.7  |     88.7    |
|      BlueLM-V-3B      |           -           | 2024.11.29 |      2.7      |      3.1     |        66.1        |    -    |   45.1   |     829    |       60.8      |     66.7    |  61.8 |       48       |      83      |     80.5     |     78.4    |    85.3   |  62.3  |     87.8    |
|     InternVL2.5-4B    |  Qwen2.5-3B-Instruct  | 2024.12.06 |      3.09     |     3.71     |        65.1        |  2337.5 |   52.3   |     828    |       60.5      |     64.3    |  60.6 |      46.3      |     81.1     |     79.3     |     76.8    |    81.4   |  58.3  |     91.6    |
|     Baichuan-Omni     |       Unknown-7B      | 2024.10.11 |       7       |       7      |          -         |  2186.9 |   47.3   |     700    |       51.9      |     62.6    |  65.4 |      47.8      |     76.2     |     74.9     |     74.3    |     -     |    -   |      -      |
|     MiniCPM-V-2.6     |        Qwen2-7B       | 2024.08.06 |      6.5      |      8.1     |        65.2        |  2348.4 |   49.8   |     852    |       60.6      |     69.7    |   60  |      48.1      |     81.2     |      79      |     80.1    |    82.1   |  57.26 |     90.8    |
|  Qwen2-VL-7B-Instruct |        Qwen2-7B       | 2024.08.28 |      6.5      |     8.29     |         67         |  2326.8 |   54.1   |     845    |       58.2      |     70.1    |   62  |      50.6      |      83      |     80.5     |     84.3    |     83    |  60.7  |     94.5    |
|  MiniCPM-Llama3-V-2.5 |   Llama3-Instruct 8B  | 2024.05.20 |       8       |     8.54     |        58.8        |  2024.6 |   45.8   |     725    |       54.3      |     63.5    |  52.8 |      42.4      |     77.2     |     74.2     |     76.6    |    78.4   |    -   |     84.8    |
|          VITA         |      Mixtral 8x7B     | 2024.08.12 |      12.9     |     12.9     |          -         |   2097  |   47.3   |     678    |       44.9      |      59     |  41.6 |      39.7      |     74.7     |     71.4     |     71.8    |     -     |    -   |      -      |
|       GLM-4V-9B       |        GLM-4-9B       | 2024.06.04 |      8.2      |     13.9     |        59.1        |  2018.8 |   46.9   |     776    |       51.1      |      -      |   58  |      46.6      |     81.1     |     79.4     |      -      |    81.1   |  58.7  |      -      |
|   LLaVA-NeXT-Yi-34B   |         Yi-34B        | 2024.01.18 |       34      |      34      |         55         |  2006.5 |   48.8   |     574    |       40.4      |      66     |  50.7 |      34.8      |     81.1     |      79      |     69.3    |    78.9   |  51.6  |      -      |
| Qwen2-VL-72B-Instruct |       Qwen2-72B       | 2024.08.28 |     70.21     |     73.4     |        74.8        |  2482.7 |   64.5   |     877    |       70.5      |     77.8    |   74  |      58.1      |     86.5     |     86.6     |     85.5    |    88.1   |  68.3  |     96.5    |

### æ–‡æœ¬å¤„ç†èƒ½åŠ›

|                       |          |             |                                       | å¯¹è¯&æŒ‡ä»¤ |                 |        | ä¸­æ–‡&è‹±æ–‡ä»»åŠ¡ |            |       |          |  ä»£ç ä»»åŠ¡ |       | æ•°å­¦ä»»åŠ¡ |       |
|:---------------------:|:--------:|:-----------:|:-------------------------------------:|:---------:|:---------------:|:------:|:-------------:|:----------:|:-----:|:--------:|:---------:|:-----:|:--------:|:-----:|
|         models        | æŒ‡ä»¤æ¨¡å‹ |   å‘å¸ƒæ—¶é—´  | Transformerå‚æ•°é‡ ï¼ˆä¸å«emb&softmaxï¼‰ |  MT-Bench | AlignBench (ZH) | IFEval |  C-EVAL (ZH)  | CMMLU (ZH) | MMLU  | MMLU-Pro | HumanEval |  MBPP |   GSM8K  |  MATH |
| Megrez-3B-Omni        |     Y    |  2024.12.16 |                  2.3                  |    8.4    |       6.5       |  66.5  |     84.0      |    75.3    | 73.3  |   45.2   |   72.6    | 60.6  |   63.8   | 27.3  |
| Megrez-3B-Instruct    |     Y    |  2024.12.16 |                  2.3                  |   8.64    |      7.06       |  68.6  |     84.8      |    74.7    | 72.8  |   46.1   |   78.7    | 71.0  |   65.5   | 28.3  |
| Baichuan-Omni         |     Y    |  2024.10.11 |                  7.0                  |     -     |        -        |    -   |     68.9      |    72.2    |  65.3 |     -    |     -     |   -   |     -    |   -   |
| VITA                  |     Y    |  2024.08.12 |                 12.9                  |     -     |        -        |    -   |     56.7      |    46.6    | 71.0  |     -    |     -     |   -   |   75.7   |   -   |
| Qwen1.5-7B            |          |  2024.02.04 |                  6.5                  |     -     |        -        |    -   |     74.1      |    73.1    | 61.0  |   29.9   |   36.0    | 51.6  |   62.5   | 20.3  |
| Qwen1.5-7B-Chat       |     Y    |  2024.02.04 |                  6.5                  |   7.60    |      6.20       |    -   |     67.3      |      -     | 59.5  |   29.1   |   46.3    | 48.9  |   60.3   | 23.2  |
| Qwen1.5-14B           |          |  2024.02.04 |                 12.6                  |     -     |        -        |    -   |     78.7      |    77.6    | 67.6  |     -    |   37.8    | 44.0  |   70.1   | 29.2  |
| Qwen1.5-14B-Chat      |     Y    |  2024.02.04 |                 12.6                  |    7.9    |        -        |    -   |       -       |      -     |   -   |     -    |     -     |   -   |     -    |   -   |
| Qwen2-7B              |          |  2024.06.07 |                  6.5                  |     -     |        -        |    -   |     83.2      |    83.9    | 70.3  |   40.0   |   51.2    | 65.9  |   79.9   | 44.2  |
| Qwen2-7b-Instruct     |     Y    |  2024.06.07 |                  6.5                  |   8.41    |      7.21       |  51.4  |     80.9      |    77.2    | 70.5  |   44.1   |   79.9    | 67.2  |   85.7   | 52.9  |
| Qwen2.5-3B-Instruct   |     Y    |  2024.9.19  |                  2.8                  |     -     |        -        |    -   |       -       |      -     |   -   |   43.7   |   74.4    | 72.7  |   86.7   | 65.9  |
| Qwen2.5-7B            |          |  2024.9.19  |                  6.5                  |     -     |        -        |    -   |       -       |      -     | 74.2  |   45.0   |   57.9    | 74.9  |   85.4   | 49.8  |
| Qwen2.5-7B-Instruct   |     Y    |  2024.09.19 |                  6.5                  |   8.75    |        -        |  74.9  |       -       |      -     |   -   |   56.3   |   84.8    | 79.2  |   91.6   | 75.5  |
| Llama-3.1-8B          |          |  2024.07.23 |                  7.0                  |    8.3    |       5.7       |  71.5  |     55.2      |    55.8    | 66.7  |   37.1   |     -     |   -   |   84.5   | 51.9  |
| Llama-3.2-3B          |          |  2024.09.25 |                  2.8                  |     -     |        -        |  77.4  |       -       |      -     | 63.4  |     -    |     -     |   -   |   77.7   | 48.0  |
| Phi-3.5-mini-instruct |     Y    |  2024.08.23 |                  3.6                  |    8.6    |       5.7       |  49.4  |     46.1      |    46.9    | 69.0  |   47.4   |   62.8    | 69.6  |   86.2   | 48.5  |
| MiniCPM3-4B           |     Y    |  2024.09.05 |                  3.9                  |   8.41    |      6.74       |  68.4  |     73.6      |    73.3    | 67.2  |     -    |   74.4    | 72.5  |   81.1   | 46.6  |
| Yi-1.5-6B-Chat        |     Y    |  2024.05.11 |                  5.5                  |   7.50    |      6.20       |    -   |     74.2      |    74.7    | 61.0  |     -    |   64.0    | 70.9  |   78.9   | 40.5  |
| GLM-4-9B-chat         |     Y    |  2024.06.04 |                  8.2                  |   8.35    |      7.01       |  64.5  |     75.6      |    71.5    | 72.4  |     -    |   71.8    |   -   |   79.6   | 50.6  |
| Baichuan2-13B-Base    |          |  2023.09.06 |                 12.6                  |     -     |      5.25       |    -   |     58.1      |    62.0    | 59.2  |     -    |   17.1    | 30.2  |   52.8   | 10.1  |

- Qwen2-1.5Bæ¨¡å‹çš„æŒ‡æ ‡åœ¨è®ºæ–‡å’ŒQwen2.5æŠ¥å‘Šä¸­ç‚¹æ•°ä¸ä¸€è‡´ï¼Œå½“å‰é‡‡ç”¨åŸå§‹è®ºæ–‡ä¸­çš„ç²¾åº¦

### è¯­éŸ³ç†è§£èƒ½åŠ›

|       Model      |     Base model     | Realease Time | Fleurs test-zh | WenetSpeech test_net | WenetSpeech test_meeting |
|:----------------:|:------------------:|:-------------:|:--------------:|:--------------------:|:------------------------:|
| Whisper-large-v3 |          -         |   2023.11.06  |      12.4      |         17.5         |           30.8           |
|  Qwen2-Audio-7B  |      Qwen2-7B      |   2024.08.09  |        9       |          11          |           10.7           |
|  Baichuan2-omni  |     Unknown-7B     |   2024.10.11  |        7       |          6.9         |            8.4           |
|       VITA       |    Mixtral 8x7B    |   2024.08.12  |        -       |      -/12.2(CER)     |        -/16.5(CER)       |
|  Megrez-3B-Omni  | Megrez-3B-Instruct |   2024.12.16  |      10.8      |         45.08        |           16.44          |

### é€Ÿåº¦

|                | image_tokens | prefill (tokens/s) | decode (tokens/s) |
|----------------|:------------:|:------------------:|:-----------------:|
| Megrez-3B-Omni |      448     |       6312.66      |       1294.9      |
| Qwen2-VL-2B    |     1378     |       7349.39      |       685.66      |
| MiniCPM-V-2_6  |      448     |       2167.09      |       452.51      |

å®éªŒè®¾ç½®ï¼š

- æµ‹è¯•ç¯å¢ƒä¸ºNVIDIA H100ä¸‹VLLMä¸‹è¾“å…¥128ä¸ªText tokenå’Œä¸€å¼  720*1480çš„å›¾ç‰‡ï¼Œè¾“å‡º128ä¸ªtokenï¼Œnum_seqså›ºå®šä¸º8ã€‚
- Qwen2-VL-2Bçš„åœ¨æ­¤å®éªŒä¸‹çš„decodeé€Ÿåº¦å°äºMegrez-3B-Omniï¼Œè™½ç„¶å…¶å…·å¤‡æ›´å°çš„åŸºåº§LLMï¼Œä½†æ˜¯ç¼–ç ä¸Šè¿°å¤§å°å›¾ç‰‡åçš„image_tokenç›¸è¾ƒMegrez-3B-Omniè¾ƒå¤šï¼Œå½±å“å®é™…æ¨ç†é€Ÿåº¦ã€‚

## å®‰è£…

ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å®‰è£…ä¾èµ–ï¼š

```shell
pip install -r requirements.txt
```

## å¾®è°ƒæ¨¡å‹

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªåŸºäº [DeepSpeed](https://github.com/microsoft/DeepSpeed) å’Œ [accelerate](https://github.com/huggingface/accelerate) çš„[å¾®è°ƒç¤ºä¾‹](./finetune/)ã€‚

### æ•°æ®å‡†å¤‡

æˆ‘ä»¬åŸºäº[ALLaVA-4V/allava_laion](https://huggingface.co/datasets/FreedomIntelligence/ALLaVA-4V/tree/main/allava_laion)æ„é€ äº†ä¸€ä¸ªç¤ºä¾‹æ•°æ®é›†ï¼š

- **å¯¹è¯**ï¼š[data/train/records.jsonl](./data/train/records.jsonl)
- **å›¾ç‰‡**ï¼š[data/train/images](./data/train/images)
- **éŸ³é¢‘**ï¼š[data/train/audio](./data/train/audio)ï¼Œæ˜¯é€šè¿‡å°†å¯¹è¯ä¸­çš„æ–‡æœ¬ä½¿ç”¨TTSè½¬æ¢ä¸ºè¯­éŸ³å¾—åˆ°çš„ã€‚

æ‚¨ä¹Ÿå¯ä»¥æŒ‰ç…§ä¸Šè¿°æ ¼å¼å‡†å¤‡è‡ªå·±çš„æ•°æ®é›†ã€‚

### ä¾èµ–å®‰è£…

```shell
pip install deepspeed accelerate
```

### å…¨å‚å¾®è°ƒ

ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤è¿è¡Œæˆ‘ä»¬çš„å¾®è°ƒç¤ºä¾‹ï¼Œè¯·æ³¨æ„å°†è„šæœ¬ä¸­çš„æ¨¡å‹è·¯å¾„æ›¿æ¢æˆæ‚¨ä¸‹è½½çš„æ¨¡å‹è·¯å¾„ã€‚

```shell
cd finetune

sh finetune.sh
```

æ‚¨å¯ä»¥é€šè¿‡è®¾ç½®`tune_vision_encoder`ã€`tune_vision_proj`ã€`tune_llm`ã€`tune_audio_encoder`ã€`tune_audio_proj`æ¥é€‰æ‹©éœ€è¦å¾®è°ƒçš„æ¨¡å—ã€‚

### æ³¨æ„äº‹é¡¹

- æ¨èä½¿ç”¨è‡³å°‘2å¼ æ‹¥æœ‰80Gæ˜¾å­˜çš„GPUè¿›è¡Œå¾®è°ƒã€‚
- åœ¨æ˜¾å­˜ä¸è¶³çš„æƒ…å†µä¸‹ï¼š
  - è¯·å°è¯•è°ƒæ•´`model_max_length`å’Œ`per_device_train_batch_size`ã€‚
  - è¯·å°è¯•å…³é—­éœ€è¦å¾®è°ƒçš„æ¨¡å—ä»¥ä¾¿å‡å°‘æ˜¾å­˜å ç”¨ã€‚
  - è¯·å°è¯•è°ƒæ•´deepspeedçš„`zero_optimization`å‚æ•°æ¥ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨ã€‚

## æ¨ç†

### ä½¿ç”¨å¤šæ¨¡æ€æ•°æ®è¿›è¡Œå¤šè½®å¯¹è¯

è¯·ä½¿ç”¨å¦‚ä¸‹è„šæœ¬è¿›è¡Œæ¨ç†ã€‚è¯·å°† `PATH_TO_PRETRAINED_MODEL` æ›¿æ¢ä¸ºä¸‹è½½çš„æ¨¡å‹æƒé‡çš„è·¯å¾„ã€‚

```python
import torch
from transformers import AutoModelForCausalLM

path = "{{PATH_TO_PRETRAINED_MODEL}}"  # æ›´æ”¹ä¸ºæ¨¡å‹çš„è·¯å¾„

model = (
    AutoModelForCausalLM.from_pretrained(
        path,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
    )
    .eval()
    .cuda()
)

messages = [
    {
        "role": "user",
        "content": {
            "text": "Please describe the content of the image.",
            "image": "./data/sample_image.jpg",
        },
    },
]

MAX_NEW_TOKENS = 100
response = model.chat(
    messages,
    sampling=False,
    max_new_tokens=MAX_NEW_TOKENS,
)
print(response)
```

å®Œæ•´çš„ç¤ºä¾‹è§ï¼š[example_chat_hf.py](example_chat_hf.py).

### ä½¿ç”¨ vLLM è¿›è¡Œæ¨ç†

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªåŸºäº vLLM æ¡†æ¶çš„æ¨ç†å‚è€ƒå®ç°ã€‚æ‚¨å¯ä»¥åœ¨ [vllm_demo/megrezo.py](vllm_demo/megrezo.py) ä¸­æ‰¾åˆ°æ¨¡å‹å®šä¹‰ã€‚

æ¨ç†æ­¥éª¤å¦‚ä¸‹ï¼š

1. å®‰è£… vLLM

æ³¨æ„ï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…ç‰¹å®šç‰ˆæœ¬çš„ä¾èµ–ï¼š

```shell
pip install vllm==0.6.3.post1 flash_attn==2.5.8 xformers==0.0.27.post2
```

2. è¿è¡Œæ¨ç†è„šæœ¬

vLLM å°šæœªæ­£å¼æ”¯æŒ MegrezOï¼Œå› æ­¤æ‚¨éœ€è¦å…ˆå¯¼å…¥æˆ‘ä»¬å®šä¹‰çš„æ¨¡å—ï¼š

```python
from vllm import ModelRegistry
from megrezo import MegrezOModel

ModelRegistry.register_model("MegrezO", MegrezOModel)
```

ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç è¿è¡Œæ¨ç†ï¼š

```python
from PIL import Image
from vllm import LLM
from vllm import SamplingParams


model_path = "{{PATH_TO_HF_PRETRAINED_MODEL}}"  # æ›´æ”¹ä¸ºæ¨¡å‹çš„è·¯å¾„
llm = LLM(
    model_path,
    trust_remote_code=True,
    gpu_memory_utilization=0.5,
)

sampling_params = SamplingParams(
    temperature=0,
    max_tokens=1000,
    repetition_penalty=1.2,
    stop=["<|turn_end|>", "<|eos|>"],
)

img = Image.open("../data/sample_image.jpg")

conversation = [
    {
        "role": "user",
        "content": {
            "text": "å›¾ç‰‡çš„å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "image": img,
        },
    },
]

# å°†å¯¹è¯è½¬æ¢ä¸º vLLM å¯æ¥å—çš„æ ¼å¼ã€‚
prompt = llm.get_tokenizer().apply_chat_template(
    conversation,
    tokenize=False,
    add_generation_prompt=True,
)
vllm_inputs = [
    {
        "prompt": prompt,
        "multi_modal_data": {
            "image": img,
        },
    }
]

# ç”Ÿæˆè¾“å‡º
outputs = llm.generate(
    vllm_inputs,
    sampling_params,
)

# æ‰“å°è¾“å‡º
for output in outputs:
    print(output.outputs[0].text)
```

å®Œæ•´çš„ç¤ºä¾‹è§ï¼š[vllm_demo/example_infer_vllm.py](vllm_demo/example_infer_vllm.py).

## ä½¿ç”¨ Gradio ä¸ MegrezO å¯¹è¯

æˆ‘ä»¬æä¾›åŸºäº Hugging Face Gradio <a href='https://github.com/gradio-app/gradio'><img src='https://img.shields.io/github/stars/gradio-app/gradio'></a> å®ç°çš„åœ¨çº¿å’Œæœ¬åœ° Demoã€‚

### åœ¨çº¿ Demo

æ¬¢è¿è¯•ç”¨åœ¨çº¿ Demo: {{TBD}}ã€‚

### æœ¬åœ° Demo
  
ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤éƒ¨ç½²æœ¬åœ° Gradio åº”ç”¨ï¼š

1. å®‰è£…ä¾èµ–:

```shell
pip install -r requirements.txt
```

2. å¯åŠ¨ Gradio åº”ç”¨

æ‚¨éœ€è¦åœ¨å‘½ä»¤è¡Œä¸­æŒ‡å®š `model_path` å’Œ `port`ã€‚`model_path` æ˜¯æ¨¡å‹çš„è·¯å¾„ï¼Œ`port` æ˜¯æœ¬åœ°æœåŠ¡å™¨çš„ç«¯å£å·ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œ`port` æ˜¯ `7860`ã€‚

```shell
python gradio_app.py --model_path {model_path} --port {port}
```

ç„¶åï¼Œæ‚¨å¯ä»¥åœ¨æµè§ˆå™¨ä¸­è®¿é—® `http://localhost:7860` ä¸æ¨¡å‹å¯¹è¯ã€‚

å¦‚éœ€è‡ªå®šä¹‰è¾“å…¥å’Œè¾“å‡ºæ¥å£ï¼Œè¯·ä¿®æ”¹ `gradio_app.py`ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è€ƒ [Gradio æ–‡æ¡£](https://gradio.app/docs)ã€‚

## å¼€æºåè®®åŠä½¿ç”¨å£°æ˜

- **åè®®**ï¼šæœ¬ä»“åº“ä¸­ä»£ç ä¾ç…§ [Apache-2.0](https://www.apache.org/licenses/LICENSE-2.0) åè®®å¼€æº
- **å¹»è§‰**ï¼šå¤§æ¨¡å‹å¤©ç„¶å­˜åœ¨å¹»è§‰é—®é¢˜ï¼Œç”¨æˆ·ä½¿ç”¨è¿‡ç¨‹ä¸­è¯·å‹¿å®Œå…¨ç›¸ä¿¡æ¨¡å‹ç”Ÿæˆçš„å†…å®¹ã€‚è‹¥ç”¨æˆ·æƒ³è·å–æ›´ç¬¦åˆäº‹å®çš„ç”Ÿæˆå†…å®¹ï¼Œæ¨èåˆ©ç”¨æˆ‘ä»¬çš„WebSearchåŠŸèƒ½ï¼Œè¯¦è§ [xxxx]ã€‚
- **æ•°å­¦&æ¨ç†**ï¼šå°æ¨¡å‹åœ¨æ•°å­¦å’Œæ¨ç†ä»»åŠ¡ä¸Šæ›´å®¹æ˜“å‡ºé”™è¯¯çš„è®¡ç®—è¿‡ç¨‹æˆ–æ¨ç†é“¾æ¡ï¼Œä»è€Œå¯¼è‡´æœ€ç»ˆç»“æœé”™è¯¯ã€‚ç‰¹åˆ«çš„ï¼Œå°æ¨¡å‹çš„è¾“å‡ºsoftmaxåˆ†å¸ƒç›¸æ¯”å¤§æ¨¡å‹æ˜æ˜¾ä¸å¤Ÿsharpï¼Œåœ¨è¾ƒé«˜temperatureä¸‹æ›´å®¹æ˜“å‡ºç°å¤šæ¬¡æ¨ç†ç»“æœä¸ä¸€è‡´çš„é—®é¢˜ï¼Œåœ¨æ•°å­¦/æ¨ç†ç­‰ç¡®å®šæ€§é—®é¢˜ä¸Šæ›´ä¸ºæ˜æ˜¾ã€‚æˆ‘ä»¬æ¨èåœ¨è¿™ç±»é—®é¢˜ä¸Šï¼Œè°ƒä½temperatureï¼Œæˆ–å°è¯•å¤šæ¬¡æ¨ç†éªŒè¯ã€‚
- **System Prompt**ï¼šå’Œç»å¤§å¤šæ•°æ¨¡å‹ä¸€æ ·ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­chat_templateé»˜è®¤çš„system promptï¼Œä»¥è·å¾—ç¨³å®šå’Œå¹³è¡¡çš„ä½“éªŒã€‚æœ¬æ¬¡æ¨¡å‹å‘å¸ƒå¼±åŒ–äº†è§’è‰²æ‰®æ¼”ç­‰æ¶‰åŠç‰¹å®šé¢†åŸŸåº”ç”¨æ–¹é¢çš„èƒ½åŠ›ï¼Œç”¨æˆ·è‹¥æœ‰ç‰¹å®šé¢†åŸŸçš„åº”ç”¨éœ€æ±‚ï¼Œæˆ‘ä»¬æ¨èåœ¨æœ¬æ¨¡å‹åŸºç¡€ä¸ŠæŒ‰éœ€è¿›è¡Œé€‚å½“å¾®è°ƒã€‚
- **ä»·å€¼è§‚åŠå®‰å…¨æ€§**ï¼šæœ¬æ¨¡å‹å·²å°½å…¨åŠ›ç¡®ä¿è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ•°æ®çš„åˆè§„æ€§ï¼Œä½†ç”±äºæ•°æ®çš„å¤§ä½“é‡åŠå¤æ‚æ€§ï¼Œä»æœ‰å¯èƒ½å­˜åœ¨ä¸€äº›æ— æ³•é¢„è§çš„é—®é¢˜ã€‚å¦‚æœå‡ºç°ä½¿ç”¨æœ¬å¼€æºæ¨¡å‹è€Œå¯¼è‡´çš„ä»»ä½•é—®é¢˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®å®‰å…¨é—®é¢˜ã€å…¬å…±èˆ†è®ºé£é™©ï¼Œæˆ–æ¨¡å‹è¢«è¯¯å¯¼ã€æ»¥ç”¨ã€ä¼ æ’­æˆ–ä¸å½“åˆ©ç”¨æ‰€å¸¦æ¥çš„ä»»ä½•é£é™©å’Œé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚

## è”ç³»æˆ‘ä»¬

![wechat](assets/wechat.jpg)
